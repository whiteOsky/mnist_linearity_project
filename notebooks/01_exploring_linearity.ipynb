{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623eee00",
   "metadata": {},
   "source": [
    "# 01 Â· Exploring the Limits of Linearity on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0bf066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -r ../requirements.txt\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "from src.data import get_mnist_dataloaders\n",
    "from src.models import LinearClassifier, MLP\n",
    "from src.utils import get_device, select_loss, accuracy_from_logits, to_onehot\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b19eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = get_mnist_dataloaders(batch_size=256)\n",
    "next(iter(train_loader))[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69573802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_experiment(model, loss_name='mse', epochs=2, lr=1e-3):\n",
    "    criterion = select_loss(loss_name)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); tl, ta, count = 0.0, 0.0, 0\n",
    "        for x, y in tqdm(train_loader, leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            if loss_name == 'mse':\n",
    "                y_oh = to_onehot(y, 10).to(device)\n",
    "                loss = criterion(torch.softmax(logits, dim=1), y_oh)\n",
    "            else:\n",
    "                loss = criterion(logits, y)\n",
    "            loss.backward(); optimizer.step()\n",
    "            b = y.size(0)\n",
    "            tl += loss.item()*b; ta += accuracy_from_logits(logits, y)*b; count += b\n",
    "        history['train_loss'].append(tl/count); history['train_acc'].append(ta/count)\n",
    "\n",
    "        model.eval(); vl, va, count = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                if loss_name == 'mse':\n",
    "                    y_oh = to_onehot(y, 10).to(device)\n",
    "                    loss = criterion(torch.softmax(logits, dim=1), y_oh)\n",
    "                else:\n",
    "                    loss = criterion(logits, y)\n",
    "                b = y.size(0)\n",
    "                vl += loss.item()*b; va += accuracy_from_logits(logits, y)*b; count += b\n",
    "        history['val_loss'].append(vl/count); history['val_acc'].append(va/count)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stage 1: Linear + MSE\n",
    "model = LinearClassifier().to(device)\n",
    "hist_linear_mse = run_experiment(model, loss_name='mse', epochs=2)\n",
    "hist_linear_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437671ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stage 2: Linear + CrossEntropy\n",
    "model = LinearClassifier().to(device)\n",
    "hist_linear_ce = run_experiment(model, loss_name='crossentropy', epochs=2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(hist_linear_mse['val_loss'], label='Linear MSE')\n",
    "plt.plot(hist_linear_ce['val_loss'], label='Linear CE')\n",
    "plt.legend(); plt.title('Validation Loss'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stage 3: MLP + ReLU\n",
    "model = MLP(hidden=256).to(device)\n",
    "hist_mlp = run_experiment(model, loss_name='crossentropy', epochs=2)\n",
    "plt.figure()\n",
    "plt.plot(hist_mlp['val_loss'], label='MLP + ReLU (CE)')\n",
    "plt.legend(); plt.title('Validation Loss'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize linear weights as 28x28 templates\n",
    "model = LinearClassifier().to(device)\n",
    "_ = run_experiment(model, loss_name='crossentropy', epochs=1)\n",
    "W = model.fc.weight.detach().cpu().numpy()\n",
    "\n",
    "import math\n",
    "cols = 5\n",
    "rows = math.ceil(10/cols)\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(W[i].reshape(28,28))\n",
    "    plt.axis('off'); plt.title(str(i))\n",
    "plt.suptitle('Linear Class Templates (Weights)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
